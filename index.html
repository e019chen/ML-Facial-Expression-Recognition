<!DOCTYPE html>
<html>
<link rel="stylesheet" href="styles.css">
<head>
    <title>Facial Expression Recognition Metrics</title>
</head>
<body>

<div class="container">
  <section class="quantitative-metrics">
    <h1>Quantitative Metrics</h1>
    <div class="metric"><span class="metric-title">Accuracy</span> = (True Positive + True Negative) / (Total Population)</div>
    <div class="metric"><span class="metric-title">Precision</span> = True Positive / (True Positive + False Positive)</div>
    <div class="metric"><span class="metric-title">Recall</span> = True Positive / (True Positive + False Negative)</div>
    <div class="metric"><span class="metric-title">F1 Score</span> = 2 * (precision * recall) / (precision + recall)</div>
  </section>

  <section class="confusion-matrix">
    <h1>Confusion Matrix</h1>
    <table>
      <thead>
        <tr>
          <th></th>
          <th colspan="2">Predicted</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="2">Actual</th>
          <td class="true-positive">True Positive</td>
          <td class="false-positive">False Positive</td>
        </tr>
        <tr>
          <td class="false-negative">False Negative</td>
          <td class="true-negative">True Negative</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section class="project-goals">
    <h1>Project Goals</h1>
    <ul>
      <li>High overall accuracy to ensure correct facial expression identification.</li>
      <li>Balanced precision and recall for each facial expression category.</li>
      <li>Minimal bias to perform well across various contexts.</li>
    </ul>
  </section>

  <section class="expected-results">
    <h1>Expected Results</h1>
    <ul>
      <li>Accuracy between 60–90%, varying with model complexity.</li>
      <li>Higher difficulty in recognizing certain expressions, like smiles and smirks.</li>
      <li>Performance limitations under certain conditions such as low-light environments.</li>
    </ul>
  </section>

  <section class="proposal-contributions">
    <h1>Proposal Contributions</h1>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Proposal Contributions</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>Chris</td><td>Introduction, background, problem definition</td></tr>
        <tr><td>Andrew H</td><td>Method</td></tr>
        <tr><td>Andrew G</td><td>Method, github repository and pages</td></tr>
        <tr><td>Edison</td><td>Method</td></tr>
        <tr><td>Euan</td><td>Result, discussion, Gantt Chart, references</td></tr>
      </tbody>
    </table>
  </section>
  
  <section class="references">
    <h1>References</h1>
    <ol>
      <li>[1] B. Büdendenbender, T. T. A. Höfling, A. B. M. Gerdes, and G. W. Alpers, “Training machine learning algorithms for automatic facial coding: The role of emotional facial expressions’ prototypicality,” PLOS ONE, vol. 18, no. 2, p. e0281309, Feb. 2023, doi: https://doi.org/10.1371/journal.pone.0281309.</li>
      <li>[2] V. Hossur, “Facial Emotion Detection Using Convolutional Neural Networks,” IEEE Xplore, Oct. 2022, Accessed: Jan. 21, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/9972510</li>
      <li>[3] “Facial Expression Recognition (FER) on AffectNet,” paperswithcode.com. https://paperswithcode.com/sota/facial-expression-recognition-on-affectnet</li>
      <li>[4] S. Zhang, H. Teng, Y. Zhang, Y. Wang, and Z. Song, “A Dual-Direction Attention Mixed Feature Network for Facial Expression Recognition,” Electronics, vol. 12, no. 17, pp. 3595–3595, Aug. 2023, doi: https://doi.org/10.3390/electronics12173595.</li>
      <li>[5] T. Debnath, M. M. Reza, A. Rahman, A. Beheshti, S. S. Band, and H. Alinejad-Rokny, “Four-layer ConvNet to facial emotion recognition with minimal epochs and the significance of data diversity,” Scientific Reports, vol. 12, no. 1, p. 6991, Apr. 2022, doi: https://doi.org/10.1038/s41598-022-11173-0.</li>
    </ol>
  </section>
</div>

</body>
</html>